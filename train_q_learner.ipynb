{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8905775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a72dc",
   "metadata": {},
   "source": [
    "# Using the PettingZoo environment\n",
    "\n",
    "This notebook provides smalls chunks of code to get you started with the Connect4 project. You do not have to use this code in you final file, but you can if you wish to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913932ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa7b137",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here are some implementations of trivial agents that you should be able to beat ultimately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d39518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.play_leftmost import PlayLeftmostLegal\n",
    "from agents.random import RandomPlayer\n",
    "from agents.malynx_deep import MalynxDeep, MalynxWithoutBlunder\n",
    "from agents.q_learner import QLearningAgent\n",
    "from agents.human import HumanPlayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d63c8f",
   "metadata": {},
   "source": [
    "We import the pre-trained Q-table for Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337f20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "q_learning_agent = QLearningAgent()\n",
    "with open(\"training/agent_q_learner.pkl\", 'rb') as f:\n",
    "    q_learning_agent.q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04094490",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomPlayer()\n",
    "leftmost_agent = PlayLeftmostLegal()\n",
    "malynx_deep_agent = MalynxDeep()\n",
    "malynx_without_blunder_agent = MalynxWithoutBlunder()\n",
    "human_agent = HumanPlayer(name= \"Human\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40c23aef",
   "metadata": {},
   "source": [
    "# Let's train the QLearner agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "699b2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvAgainstPolicy: \n",
    "    def __init__(self, env, policy, first_player=True):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.first_player = first_player\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step(action)\n",
    "        obs, reward, terminated, _, _ = self.env.last()\n",
    "        if terminated: \n",
    "            self.last_step = obs, reward, True, False, {}\n",
    "        else: \n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "            obs, reward, terminated, _, _ = self.env.last()\n",
    "            self.last_step = obs, -reward, terminated, False, {}\n",
    "        return self.last_step\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        if not(self.first_player): \n",
    "            # CAD 1rst player is q-learner\n",
    "            obs, _, _, _, _ = self.env.last()\n",
    "            action = self.policy.get_action(obs)\n",
    "            self.env.step(action)\n",
    "\n",
    "        self.last_step = self.env.last()\n",
    "        return self.last_step\n",
    "\n",
    "    def last(self):\n",
    "        return self.last_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf14f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_game(agent, opponent, first_player=True):\n",
    "    eval_env = EnvAgainstPolicy(env, opponent, first_player=first_player)\n",
    "    done = False\n",
    "    eval_env.reset()\n",
    "    obs, _, _, _, _ = eval_env.last()\n",
    "    while not done:\n",
    "        # We get the action from the agent\n",
    "        action = agent.get_action(obs, epsilon=0)\n",
    "        if action is None:\n",
    "            # The agent cannot play: draw?\n",
    "            return 0\n",
    "        # print(obs['action_mask'],list(np.where(obs['action_mask'] ==1)[0]), action)\n",
    "        # We move according to the action\n",
    "        eval_env.step(action)\n",
    "        next_obs, reward, done, _, _ = eval_env.last()\n",
    "        # We update the agent's Q-table\n",
    "        agent.update(obs, action, reward, done, next_obs)\n",
    "\n",
    "        if done and reward==1:\n",
    "            # The agent won\n",
    "            return 1\n",
    "        elif done and reward==-1:\n",
    "            # The agent lost\n",
    "            return -1\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "    # The game ended in a draw\n",
    "    return 0\n",
    "\n",
    "def train(agent, opponent, N_episodes=10, N_games=100, first_player=True, alternate_first_player=False):\n",
    "    # We print the evolution of the agent's statistics, and at the end we plot the evolution of the win rate\n",
    "    print(f\"{'Episode':<10} {'Win':<10} {'Loss':<10} {'Draw':<10}\")\n",
    "    win_rate = []\n",
    "    for i in range(N_episodes):\n",
    "        win = 0\n",
    "        loss = 0\n",
    "        draw = 0\n",
    "        for _ in range(N_games):\n",
    "            result = train_one_game(agent, opponent, first_player=first_player)\n",
    "            if result == 1:\n",
    "                win += 1\n",
    "            elif result == -1:\n",
    "                loss += 1\n",
    "            else:\n",
    "                draw += 1\n",
    "        print(f\"{i:<10} {win:<10} {loss:<10} {draw:<10}\")\n",
    "        win_rate.append(win / N_games)\n",
    "\n",
    "        if alternate_first_player:\n",
    "            first_player = not first_player\n",
    "\n",
    "    plt.plot(win_rate)\n",
    "    #calculate equation for trendline\n",
    "    x = np.linspace(0, N_episodes-1, N_episodes)\n",
    "    z = np.polyfit(x, win_rate, 1)\n",
    "    p = np.poly1d(z)\n",
    "    #add trendline to plot\n",
    "    plt.plot(x, p(x))\n",
    "    #legends\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win rate\")\n",
    "    plt.title(\"Evolution of the win rate for {} against {}\".format(agent.name, opponent.name))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3efd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_agent(agent, name):\n",
    "    # Save the agent's Q-table\n",
    "    # We try to optimize the size of the file by removing the useless entries\n",
    "    q_table = {}\n",
    "    for key, value in agent.q_table.items():\n",
    "        if value != 0:\n",
    "            q_table[key] = value\n",
    "    \n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(q_table, f)\n",
    "\n",
    "def load_agent(name):\n",
    "    # Load the agent's Q-table\n",
    "    agent = QLearningAgent()\n",
    "    with open(name, 'rb') as f:\n",
    "        agent.q_table = pickle.load(f)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebeba1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_against_opponent(opponent):\n",
    "    print(\"Training against {}\".format(opponent.name))\n",
    "    train(q_learning_agent, opponent, N_episodes=100, N_games=100, first_player=True)\n",
    "    train(q_learning_agent, opponent, N_episodes=250, N_games=100, first_player=False)\n",
    "    train(q_learning_agent, q_learning_agent, N_episodes=100, N_games=100, first_player=False)\n",
    "    train(q_learning_agent, opponent, N_episodes=100, N_games=100, first_player=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee1ccffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_against_opponent(random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d35e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_against_opponent(malynx_deep_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1d6cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    Win        Loss       Draw      \n"
     ]
    }
   ],
   "source": [
    "# from interface import train_from_human\n",
    "\n",
    "# train_from_human(env, q_learning_agent)  -> RUN DIRECTLY THE INTERFACE.PY FILE AS IS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2a939b1",
   "metadata": {},
   "source": [
    "# Let's save our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f4695",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "save_agent(q_learning_agent, \"training/agent_q_learner_1.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
